---
tags: [test, oop, functional-programming]
---

# Test Driven Development (TDD)

TDD is a software development practice that turn software requirements into test
cases before their actual implementation incrementally. It encourages the
developers to write the test at the same time or even before the writing of the
production code. This has the advantage of locating bugs earlier, which is by
itself cheaper to fix compare to if the bugs are found later. Some communities
such as Smalltalk and eXtreme Programming refer this practice as *code a little,
test a little* or *continuous integration, relentless testing*. To build
successful test cases, we can follow the [Testing Principles](202305211425.md).

The codes are not done if only the implementation of them are written into the
codebase. The code can be said is done **after all the tests ran**. If the tests
can't be done on a component, this means that the component is not modular
enough. [Decouple](202202041514.md)# it further to make rooms for more modular
testing.

**Note**: If a tester found a bug, build a test for that bug. Once a human
tester finds a bug, it should be *the last time a human tester finds that bug*.

[Unit Testing](202206201320.md)# is the foundation of this whole practice. That
being said, other testing such as [Integration Testing](202206201330.md)#,
[Validation and Verification of System Requirements](202206201346.md)#,
[Environmental Testing](202206201437.md)#, [Performance Testing](202206201415.md)#,
[Usability Testing](202206201428.md)# and [Runtime Diagnostics](202206301924.md)#
should not be underestimated. All test cases should honour its
[contract](202206301938.md) where we can check whether the code meet the
contract of the contract means what we think it means.

Both Unit Testing and Integration Testing should be composable; that is, a test
can be composed of subtests of subcomponents or subsystems to any depth, so that
we can easily select which kind of tests that we should run without involving
the whole code base. You can add in [Regression Testing](202206201335.md)# to
enforce
this practice.

Run all the tests that doesn't require
[specialised equipment or environment](202206201415.md)#
([Unit Testing](202206201320.md)# and [Integration Testing](202206201330.md)#)
before checking the code. We should do it often and automatically, which could
be done by incorporating the [testing facility](202206091002.md)# into the
project build system such as CMake and Makefile. If a full build is necessary,
make sure all available tests are running during the process.

[Hunt et al.](lit/@Hunt1999.md) recommends that both
[Validation and Verification of System Requirements](202206201346.md)# and
[Usability Testing](202206201428.md)# should be done as early as possible. And
that [Regression Testing](202206201335.md)# to be incorporated into the build
system in order to compare it to the previous build.

**Note**: Make sure test facilities are easily accessible by the project team.

Since we have such extensive testing facilities, newcomers could just look at
the tests to know how to use a particular module or function. They will know
what's the expected input and output for it, and use it accordingly. Even if
they failed to understand it, the testing facilities will remind them by
throwing fail tests to them.

The test data for these testing could be collected from real-world (usually user
data coming from an existing system, a competitor's system or a prototype) or
artificially created (in order to satisfy the need of large amount of data, to
stress the boundary conditions, or exhibits certain statistical properties such
as [the runtime of an algorithm](202201171853.md)#). These data can later be
used in the [Regression Testing](202206201335.md)#.

Although more code coverage can mean a better tested codebase, a full coverage,
that is 100% code coverage is impossible. Therefore, it is advised to test the
state of the program, not the line of code.

To improve this model, the development team can hire or allocate a dedicated
test saboteur. Their sole responsibility is to sabotage the testing by
deliberately causing the bugs for the tests to catch in a separate source tree.

**Note**: Even with extensive testing, one could not avoid drawing a #[naive assumption](202207091727.md)
about a routine, resulted in false causalities and coincidental outcomes. [Hunt et al.](lit/@Hunt1999.md)
advices "Don't assume it, prove it".
