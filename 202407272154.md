---
tags: [software, performance, caching, multithreading]
---

# Software Prefetching

Software prefetching can hide memory access latency which could resulted in 8%
performance boost after exceeding [L2 cache](202403191017.md) as tested by
Drepper. Accompany by [Compiler Intrinsics](202404151959.md) `_mm_prefetch(void*
ptr, _mm_hint)` and enum `_mm_hint`, we can optimise the codebase first by
locating a large source of cache misses, adding prefetch instruction
accordingly, and checking the result by observing performance. The procedure
should be done incrementally, meaning that we should optimise one source at a
time. Note that we shall avoid unnecessary software prefetching as it can occupy
Front Side Bus (FSB) bandwidth.

GCC compiler offers the flag `-fprefetch-loop-arrays` to emit prefetch
instructions for loop that have to iterate over an array, but it is not
recommended as it is poorly performed in small array and dynamically allocated
data structure.

The intrinsic `_mm_prefetch()` instructs the unit to load data into a cache and
evict data if necessary. There are four states, generally, that can influence
the intrinsic's behaviour as defined in the enum `_mm_hint`: `__MM_HINT_T0`,
`__MM_HINT_T1`, `__MM_HINT_T2`, and `__MM_HINT_NTA`. `__MM_HINT_T0`,
`__MM_HINT_T1`, and `__MM_HINT_T2` work almost the same such that all of them
load data into all levels of cache for inclusive caches and into the lowest
level cache for exclusive caches except that they differ in behaviour when there
is a data item in a higher level cache. With `__MM_HINT_T0`, this is loaded into
L1d, `__MM_HINT_T1` loads it into L2, and `__MM_HINT_T2` loads it into L3 if
there is any. Although there can be more details on their implementation, their
specification is highly dependent on the support from the vendors. In most of
the cases, `__MM_HINT_T0` is the choice especially if the data has to be used
right away. Be aware of the size of L1d though. `__MM_HINT_NTA` incorporates
[CPU Non-Temporal Access (NTA)](202407122002.md) mechanism to avoid cache
pollution as the data is temporal (short-live). There is a caveat though, if the
working set is too large to handle and forces eviction of cache line with NTA
hint happens, then a reload from memory will occur which has performance
implication.

Processor manufacturer like AMD offers localised solution such as `prefetchw`.
This works by prefetching a cache line into L1 and put it into 'M' state. It has
the advantages when multiple writes follow after the prefetching as there is no
need to change the state of the cache line, which improve the performance.
However, if no write is followed, then this will only introduce overhead.

In general, software prefetching can make program's codebase more complex to
deal with, especially when there is a need to iterate over a data structure, in
which programmers has to determine how far should them look ahead, decreasing
runtime performance due to prefetching, and increasing code size of loop. The
solutions proposed by Drepper is to have helper threads that only do prefetch
and other simple operations which can be implemented as either hyper-threads
(for single-core processor) or dumb threads. The idea is to completely separate
normal operations and prefetching. Helper threads are mostly idle and don't
compete with normal operation thread in main memory access. [Synchronisation method](202404301021.md)
such as [Linux](202204081225.md)-specific `futex` or [POSIX primitives](202307131648.md)
can be used to guard helper threads from running too far ahead which could cause
unnecessary pollution in caches and evictions. Although it introduces
performance overhead, helper threads improve the performance 25%, as tested by
Drepper, after the working set's size exceed the size of L2 cache.
