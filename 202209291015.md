---
tags: [math]
---

# Information

The Information of an event is **inversely related** to the probability of the
event occurrence. Its unit is in *bits*. The higher the probability of the event
occurrence, the lower the information contained in the event. If the probability
is 1 or 0, that is the event will always occur or not occur respectively, there
will be no information there. In casual terms, if there is no *surprise*, no
information should be expected, and if the more *surprise* it could be (low
probability), the more information would be available to the observer. However,
there will be the case where the information contained is of negative. This
means that there is a loss of information. The formal definition is as follows:

$$
\begin{align}
I(s_k) &= 0 \text{ for } p_k = 1 \\
I(s_k) &= \lg (\frac{1}{p_k}) \ge 0 \text{ for } 0 \le p_k \le 1 \\
I(s_k) &> I(s_i) \text{ for } p_k < p_i \\
\end{align}
$$

Where:
- $I$ represents the information of the event
- $s$ represents the event
- $p$ represents the probability of the event

If the information is transmitted in a [Discrete Memoryless Source (DMS)](202209291056.md)#,
the total amount of information for a source could be calculated from summing up
all the information from the symbols.

In the realm of Information Theory, it is common to see the discussions are
steer towards on calculating blocks instead of individual symbols, where each
block consisting of $n$ successive source symbols. These are being produced by
an extended source alphabet $S^n$ that has $K^n$ distinct blocks (total
permutation) where $K$ is the number of symbols produced by $S$. There will be
different ways of how the symbols could be layout, and we'll need to find out
all the probability for each of the combination.

Before that, we need to group all of them together if they have the same value
pattern ~~(having the same symbols produced albeit in different arrangement)~~.
Then, we calculate the corresponding probability for the group. After that, we
could get its [Entropy](202209291005.md)# $H(S^n)$ by getting the summation of
the product of probability for the block group and its occurrence.
