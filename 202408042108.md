---
tags: [caching, multithreading, parallelism]
---

# Caching In Multithread Environment

In multithreading environment, there are closely-linked aspects that play
important role on optimising [CPU Cache](202403191017.md) performance, which are
[Concurrency](202202011815.md), atomicity, and bandwidth. We will look into
these aspects individually at below sections.

**Note**: Advices in multithread environment can be applied to multiprocess
environment too, just so we have to distinguish [Differences between Parallelism and Concurrency](202202011649.md).

## Concurrency

Some cache optimisations may be bad for concurrency. An example for this is
elaborated in [Cache Access Optimisation](202407191014.md) where we group common
data into a single structure. Though the data is shared among threads through
caches, it will generate a lot of request-for-ownership (RFO), each contributes
a write access, due to the cache line's ["E" state](202403271116.md) (exclusive)
as multiple write happen to the same memory location. This phenomenon is known
as **false sharing** (Drepper).

The solution is to have separate cache line for the same data for processors,
but this could come at the cost of increasing program's footprint. Drepper
developed two criteria on evaluating which kind of variable should have to be
individually owned or cached by processors: mutability and its frequency.

For constant variable (`const`), it will be shared in "S" state, so grouping
constant variables is fine. GCC compiler usually moved them into `.rodata` or
`.data.rel.ro` section.

If the variable is only ever used by one thread, we can define it as a
thread-local variable with GCC's specifier `__thread`. Such variable is
addressable by other threads via a passing pointer provided by the owner, though
they can't find it in global address space. Thread-local variable comes with the
downsides of increasing thread initialisation cost, higher addressing cost, and
higher resource cost (DSO allocation triggered when variable is used as memory
for all other thread-local variables are packed into the same object). Another
problem of using thread-local variable is that current value in the old thread
is not available to the new one if the variable is shifted to the new thread
since the copy of the variable is distinct. This can be solved through
coordination.

The general advice, by Drepper, except those that are mentioned above, is to
group read-write data that is often used together into a single structure like
in [Cache Access Optimisation](202407191014.md). Move those read-write variables
that are frequently written by different threads onto their own cache line, with
the compilation flag `-fms-extensions`. Variables that are used by multiple
thread, but each use is independent, should be moved into thread-local storage
(TLS).

## Atomicity

Simultaneous modification of a value will result in unexpected value due to
unspecified order of the threads' execution and they are fetched from cache not
the main memory. The solution to this is *atomic operation* which will read old
value only when the operation can be done atomically. The processor may signal
atomic operations for specific addresses to other devices in addition to wait
for other cores and CPUs. However, atomicity can hit performance in
multithreading environment, Drepper suggests deploying it when there is only one
thread in use.

Atomic operation can be classified into *bit test*, *Load Lock/Store Conditional
(LL/SC)* (or Linked Lock), *Compare-And-Swap (CAS)*, and *atomic arithmetic*.
Bit test set or clear a bit atomically, and then return the status indicating
whether the bit was set before. LL/SC is a pair of operations where the special
load instruction is used to start transaction, and the final store will only
succeed if the location is not modified in the meantime while also returns its
success state. CAS is a ternary operation that writes value as a parameter into
an address only if the current value is equal to the third parameter's value.
Atomic arithmetic is available only on **x86/64** which perform arithmetic and
logical operations on memory location.

**Note**: RISC architecture have fewer atomic operations comparing to CISC, as
sometimes the vendor only provides bit set and test.

The vendor of a [processor architecture](202403151651.md) have to choose between
LL/SC or CAS as an implementation to ensure atomicity for a set of instructions.
Although LL/SC performs better CAS (due to higher memory handling overhead),
according to the tests performed by Drepper, most vendor treat CAS as the
unifying atomic operation to make programs simpler. The reason that LL/SC has
performance edge over CAS is because CPU keeps them together when executing
atomic arithmetic operation and blocks concurrently-issued cache line requests
until the atomic operation is done. It defines machine abstraction at the level
which the atomic arithmetic and logical operation can be utilised.

LL/SC operations can be done by either `__sync_add_and_fetch()` (add and read
result) or `__sync_fetch_and_add()` (add and return old value) [Compiler Intrinsics](202404151959.md).
CAS operation can be done through `__sync_bool_compare_and_swap()`.

In most CPU architecture, atomic operation instruction is always instruction,
which complicates the code paths. In x86 architecture though, it utilises the
prefix `lock` to turn operation into an atomic one.

## Bandwidth
