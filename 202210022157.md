---
tags: [security, networking]
---

# Availability

Availability is the ability of the loss or a reduction in accessibility of
elements of a distributed system. By proper use of [Data Integrity](202210040913.md)#
and [Authentication Exchange](202210040915.md)#, it could counter
[Denial of Service (DoS)](202209262115.md)#.

Some would discuss Availability in terms of *24/7/365*, meaning the service will
be up 24 hours a day, 7 days a week, 365 days a year. *Nines* could be used to
refer Availability in percentage, such as *four nines* referring to *99.990%*.
However, due to the ever distributed networks, devices, and services, defect per
million (DPM) is adopted instead to measure Availability.

| Availability | Nines       | DPM    | Downtime Per Year            |
| ---          | ---         | ---    | ---                          |
| 99.000%      | Two Nines   | 10,000 | 3 days, 15 hours, 36 minutes |
| 99.900%      | Three Nines | 1,000  | 8 hours, 46 minutes          |
| 99.990%      | Four Nines  | 100    | 53 minutes                   |
| 99.999%      | Five Nines  | 10     | 5 minutes                    |

Other than these, it can be measured as the ratio between the uptime and sum of
the uptime and downtime of the system, or the percentage of agreed service time
and downtime of the system. We can also use metrics such as mean time to failure
(MTTF), mean time between failures (MTBF), and mean time to repair (MTTR) to
calculate the availability of a system.

$$
\begin{align}
\text{availability} &= \frac{\text{uptime}}{\text{uptime} + \text{downtime}}\\
\text{availability}(\%) &= \frac{\text{agreed service time} -
\text{downtime}}{\text{agreed service time}}\\
\text{availability} &= \frac{\text{MTBF}}{\text{MTBF} + \text{MTTR}}\\
\end{align}
$$

A high availability service will *prevent financial loss* and *productivity
loss*, *reduces reactive support costs* (the cost to fix when things break), and
*improves customer satisfaction and loyalty*.

The faults can be roughly categorised into *Byzantine failures*, and *crash
failures*. Byzantine failures describe failures where the system components fail
in arbitrary ways which cause the system to behave abnormally and inconsistent.
In crash failures, the system components stop functioning completely or remain
inactive during failures.

The availability of the service could be disrupted by operational errors,
network equipment failures, software failures, and security holes. Examples of
operational errors, which is typically human errors, are the result of poor
change-management processes, and lack of training and documentation. Network
equipment failures include hardware failures, power outages or service provider
outages, overheating and backhoe (type of excavating equipment or digger).
Causes of software failures could be software crashes, unsuccessful
switch-overs, or latent code failures.

Design practices could improve the availability of a service including
- hardware redundancy to avoid single point of failure
- software availability such as [Spanning Tree Protocol (STP)](202207081637.md)
  and Hot Standby Router Protocol (HSRP)
- network/server redundancy
- [link/carrier availability](202211211214.md)#
- clean implementation/cable management
- backup power/temperature management
- network monitoring *simply network design*,
- change control management (testing before applying changes)
- training
- backup/automatic recover (snapshot)
- security posture and policies
- adaptive fault-tolerance (monitors the system's state and adapt the system to
  the errors)
