---
tags: [math]
---

# Channel Coding Theorem

Channel Coding Theorem, or Shannon's second theorem is a safe or maximum limit
of transmission rate over a noisy channel without the loss of
[Information](202209291015.md)#. In fact, the probability of error should be
$10^{-6}$ or even lower so that transmission of the symbols is feasible. Channel
Coding is to increase the resistance of a digital [Communication System](202210042126.md)#
to channel noise in order to achieve high performance level. It is by increasing
the redundancy into the code.

For [Discrete Memoryless Source (DMS)](202209291056.md)#, there exists a coding
scheme where the source output can be transmitted over the channel and be
reconstructed with a very small probability of error if:

$$
\frac{H(S)}{T_S} \le \frac{C}{T_C}
$$

Where:
- $H(S)$ is the [Entropy](202209291005.md)# from a DMS with an alphabet $S$
- $T_S$ is the time interval when $S$ produces symbols
- $\frac{C}{T_C}$ is the [Critical Rate](202210290931.md)#

For [Binary Symmetric Channel](202210261256.md), since $H(S)$ is equal to 1, we
can reconstruct the formulae above to get the following formula:

$$
r \le C
$$

Where:
- $r$ is the [Information Rate](202210052143.md)# which is defined as
  $\frac{T_C}{T_S}$
- $C$ is the [Channel Capacity](202210290901.md)#

[Channel Encoder](202209291041.md)# is ~~subjected to~~ a device that practice
this theorem.
