---
tags: [hardware, memory, caching]
---

# CPU Cache

Most CPU caches are composed of [Static RAM (SRAM)](202403181035.md) due to its
relatively faster access speed when comparing to [Dynamic RAM (DRAM)](202403142052.md).

There are advantages on separating code and data caches. In fact, caching
decoded instructions can speed up execution especially during emptied pipeline
due to incorrectly predicted or impossible-to-predict branches. Most modern
[Computer Architectures](202403151651.md) have multiple level caches (usually
three) implemented.

A cache entry is tagged using the data word's virtual or physical address in
main memory. In addressing spatial locality, that is nearby memory locations are
likely to be referenced, the cache entry typically stores lines of several
contiguous words from 32 bytes up to 64 bytes depending on the vendor's
implementation. CPU finds the data word's memory address for each cache line by
masking address value according to cache line size.

Each cache line is composed of **tag**, **cache set**, and **offset**. The tag
represents that value associates with each cache line to distinguish all the
aliases which are cached in the same cache set. The same alias means that the
cache lines is off the same cache set. When we call a cache line **dirty**, it
means the cache line has been written to by the CPU but hasn't written back to
the main memory. This results in two caching strategies: *exclusive cache* and
*inclusive cache*. Exclusive cache pushes old cache line down to lower level
caches to make room for it. Each retrieval of the cache line will become
progressively more expensive. One possible advantage of implementing exclusive
cache is that loading new cache line only need to touch the first-level data
cache (L1d). Inclusive cache allows cache line to present in multiple level of
the caches, which means increasing memory requirement. However, with enough
second-level caches (L2), such disadvantage is outweighs by faster eviction.

In symmetric multiprocessor (SMP) systems, all processors should see the same
memory content at all times. This maintenance of uniform view of the memory is
called **cache coherency**. In practice, it is done by marking the cache line as
invalid when a write access is detected and the processor requesting it has the
clean copy of the cache line. More sophisticated implementation allows
requesting processor to get the dirty cache line from the first processor, and
through snooping, the first processor notices it and sends the data to it,
bypassing the main memory (though in some cases the memory controller may
notice such change and update the memory content). If the requesting processor
requests for a write access on the cache line, the first processor will instead
invalidate its local copy. One of the cache coherency protocols is MESI.

Regarding on write access, CPU doesn't need to wait during write operation as
long as the execution of the following instructions have the same effect. With
[shadow register](202403191113.md), it is even possible to change the value
which is to be stored in the incomplete write operation.

The slow access time in L2 is mainly caused by write delay, and such delay will
increase upon the increasing size of the L2 cache. This can only be improved by
process shrinking. The cost on cache load and miss can be mitigated if the
memory load operation is started early in the pipeline, which could be in
parallel to other operations. Obstacles to this improvement is either not having
sufficient resources for memory access or the final address of the load only
becomes available as the result of another instruction.

Another factor that can slow down the cache is the fact that many main memory
locations will have to fight for each piece in the cache. This is solved by
delving into [associativity.](202403202309.md)
