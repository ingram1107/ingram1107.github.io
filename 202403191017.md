---
tags: [hardware, memory, caching]
---

# CPU Cache

Most CPU caches are composed of [Static RAM (SRAM)](202403181035.md) due to its
relatively faster access speed when comparing to [Dynamic RAM (DRAM)](202403142052.md).

There are advantages on separating code and data caches. In fact, caching
decoded instructions can speed up execution especially during emptied
[pipeline](202403292159.md) due to incorrectly predicted or
impossible-to-predict branches. Most modern [Computer Architectures](202403151651.md)
have multiple level caches (usually three) implemented. The transfer of stored
instruction and data from cache to CPU or vice versa is done word size, thus
termed word transfer.

A cache entry is tagged using the data word's virtual or physical address in
main memory. In addressing spatial locality, that is nearby memory locations are
likely to be referenced, the cache entry typically stores lines of several
contiguous words from 32 bytes up to 64 bytes depending on the vendor's
implementation. CPU finds the data word's memory address for each cache line by
masking address value according to cache line size.

Each cache line is composed of **tag**, **cache set**, and **offset**. The tag
represents that value associates with each cache line to distinguish all the
aliases which are cached in the same cache set. The same alias means that the
cache lines is off the same cache set. When we call a cache line **dirty**, it
means the cache line has been written to by the CPU but hasn't written back to
the main memory. This results in two caching strategies: *exclusive cache* and
*inclusive cache*. Exclusive cache pushes old cache line down to lower level
caches to make room for it. Each retrieval of the cache line will become
progressively more expensive. One possible advantage of implementing exclusive
cache is that loading new cache line only need to touch the first-level data
cache (L1d). Inclusive cache allows cache line to present in multiple level of
the caches, which means increasing memory requirement. However, with enough
second-level caches (L2), such disadvantage is outweighs by faster eviction.

In symmetric multiprocessor (SMP) systems, all processors should see the same
memory content at all times, i.e., maintaining [sequential consistency](202403301110.md).
This maintenance of uniform view of the memory is called **cache coherency**. In
practice, it is done by marking the cache line as invalid when a write access is
detected and the processor requesting it has the clean copy of the cache line.
More sophisticated implementation allows requesting processor to get the dirty
cache line from the first processor, and through snooping, the first processor
notices it and sends the data to it, bypassing the main memory (though in some
cases the memory controller may notice such change and update the memory
content). If the requesting processor requests for a write access on the cache
line, the first processor will instead invalidate its local copy. One of the
cache coherency protocols is [MESI](202403271116.md). The biggest constraint on
cache coherency is the longest reply time from the processor. Other factors are
high bus collision, high latency in NUMA system, and traffic volume (in FSB,
memory bus, memory module), its effect on the bus bandwidth.

**Note**: Especially in multicore processor, it might choose poorly in cache
evictions in order to rebalance cache usage between cores.

Regarding on write access, CPU doesn't need to wait during write operation as
long as the execution of the following instructions have the same effect. With
[shadow register](202403191113.md), it is even possible to change the value
which is to be stored in the incomplete write operation.

The slow access time in L2 is mainly caused by write delay, and such delay will
increase upon the increasing size of the L2 cache. This can only be improved by
process shrinking (in some cases [Hardware Prefetching](202403210909.md), but it
may work in disadvantage). The cost on cache load and miss can be mitigated if
the memory load operation is started early in the pipeline, which could be in
parallel to other operations. Obstacles to this improvement is either not having
sufficient resources for memory access or the final address of the load only
becomes available as the result of another instruction.

$$
\text{Cache size} = \text{cache line size} + \text{associativity} + \text{number 
of sets}
$$

Another factor that can slow down the cache is the fact that many main memory
locations will have to fight for each piece in the cache. This is solved by
delving into [associativity.](202403202309.md)

There are several cache policies to be selected by the system so that the result
of the system after the cache line is modified is the same as if there is no
cache at all and the memory location is modified. These policies are:
- [Write-Through Cache Policy](202403271102.md)
- [Write-Back Cache Policy](202403271105.md)
- [Write-Combining Cache Policy](202403271107.md)
- [Uncacheable Cache Policy](202403271111.md)

Note that, however, once L3 cache is no longer sufficient to hold the working
set size, especially in #parallelism environment, the speed-up is so miniature
that it is simply not worth the cost and effort to add in more processors to the
motherboard.

For instructions or codes, caching is less problematic as it is generated,
usually even [optimised](202407211214.md), by the compiler in which the code
always has quite good spatial and temporal locality with its code quality fixed.
The code quality depends on the size of the code, which depends on the
complexity of the problem, which is then fixated. Thus, the program flow is more
predictable with the helps from [Prefetching](202403210909.md). However, stall
triggered by instruction flow interruption may become a concern. In Complex
Instruction Set Computer (CISC) architecture, decoding takes time, which means
that directly storing instruction into L1i cache is likely to cause stall. One
solution is to store the raw byte sequence in L2 and then stores the decoded
instructions in L1i, which is named as [trace cache](202403292155.md).

There is not much can a programmer to improve on the software level other than
try to generate small code and help the computer to make good prefetching
decisions by code layout or explicit prefetching.

The general advice on cache improvement is to minimise random access (in matrix,
avoid iterating over columns) and employ more sequential read or write as the
latter is optimised by modern CPU.
