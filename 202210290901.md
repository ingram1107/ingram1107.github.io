---
tags: [math]
---

# Channel Capacity Theorem

Channel Capacity of a [Discrete Memoryless Channel (DMC)](202210261202.md)# is
defined as the maximum average [Mutual Information](202210261345.md)# which is
measured in bits per channel use:

$$
C = I(A_X; A_Y)
$$

By Shannon's third theorem, Channel Capacity is defined as:

$$
\begin{align}
C &= B \lg (1 + \frac{P}{N_0 B}), \quad \text{or}\\
C &= B \lg (1 + \frac{E_b C}{N_0 B}), \quad \text{or}\\
C &= B \lg (1 + SNR)
\end{align}
$$

Where:
- $B$ is the channel bandwidth in Hz
- $N_0$ is the additive white Gaussian noise of power spectral density
- $P$ is the average transmitted power which $P = E_b C$ where $E$ is the
  transmitted energy per bit
- $SNR$ is the average [signal-to-noise ratio](202210290953.md)#

We can get the *signal energy-per-bit to noise power spectral density ratio*
$\frac{E_b}{N_0}$ by changing the equation:

$$
\frac{E_b}{N_0} = \frac{2^{\frac{C}{B}} - 1}{\frac{C}{B}}
$$

It is not possible to transmit at a rate higher than $C$ bits per second by any
encoding system without a definite probability of error. Thus, it defines the
fundamental limit on the rate of error-free transmission for a power-limited,
band-limited Gaussian channel. (an application of [Channel Coding Theorem](202209291049.md))
From that, we could know the [Shannon's Limit](202211032137.md)# for an Additive
White Gaussian Noise (AWGN) Channel if the bandwidth is infinite. A
[Bandwidth Efficiency Diagram](202211032128.md)# could be plotted utilising the
equation's characteristics.
